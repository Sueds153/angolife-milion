"""
AngoJobScraper v2 â€” Super Motor de Vagas de Emprego Angolanas
=============================================================
Arquitetura de 'Adaptadores' unificada com suporte a 8+ fontes.

Fontes configuradas:
  1. AngoEmprego.com     â†’ Plataforma nacional lÃ­der
  2. AngoVagas.net       â†’ WordPress, volume mÃ©dio
  3. Emprega Angola      â†’ Portal nacional moderno
  4. INEFOP              â†’ Concursos PÃºblicos e Estado
  5. Careerjet Angola    â†’ Volume de vagas classe mÃ©dia
  6. Mirantes            â†’ Talatona / Luanda Sul
  7. AngoJob.net         â†’ Portal agregador angolano
  8. LinkedIn (PÃºblico)  â†’ Vagas pÃºblicas sem login

Funcionalidades:
  âœ… JOBS_CONFIG â€” dicionÃ¡rio unificado de adaptadores
  âœ… Chrome v122 User-Agent real (anti-403/bloqueios)
  âœ… DeduplicaÃ§Ã£o dupla: por source_url E por (title + company)
  âœ… CategorizaÃ§Ã£o automÃ¡tica por palavras-chave no tÃ­tulo
  âœ… ExtraÃ§Ã£o de imagem: og:image â†’ logo img â†’ None
  âœ… ExtraÃ§Ã£o de e-mail por regex na pÃ¡gina de detalhe
  âœ… 2-5s de delay aleatÃ³rio entre requests (simulaÃ§Ã£o humana)
  âœ… Per-site try-except blindado â€” falha isolada por fonte
  âœ… Log de estatÃ­sticas completo no final

DependÃªncias:
    pip install requests beautifulsoup4 python-dotenv
"""

import re
import os
import time
import json
import random
import logging
import unicodedata
from datetime import datetime, timezone
from typing import Optional, List, Dict
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURAÃ‡ÃƒO DE LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("jobs_scraper.log", encoding="utf-8"),
    ],
)
log = logging.getLogger("AngoJobScraper")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INTELIGÃŠNCIA: CategorizaÃ§Ã£o automÃ¡tica de vagas por tÃ­tulo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_MAP = {
    "Tecnologia": [
        "IT", "TI", "InformÃ¡tica", "Developer", "Desenvolvedor", "Programador",
        "Software", "Sistemas", "Redes", "CiberseguranÃ§a", "Data", "Python", "Java",
        "Frontend", "Backend", "Fullstack", "DevOps", "Cloud", "Suporte TÃ©cnico"
    ],
    "GestÃ£o": [
        "Gerente", "Gestor", "Director", "Diretor", "Manager", "Supervisor",
        "Coordenador", "CoordenaÃ§Ã£o", "CEO", "CFO", "COO", "Chefe", "ResponsÃ¡vel"
    ],
    "FinanÃ§as": [
        "Contabilista", "Contabilidade", "Financeiro", "FinanÃ§as", "Auditor",
        "Auditoria", "Tesoureiro", "Economista", "AnÃ¡lise Financeira", "Fiscal"
    ],
    "SaÃºde": [
        "MÃ©dico", "Enfermeiro", "Enfermeira", "FarmacÃªutico", "TÃ©cnico de SaÃºde",
        "SaÃºde", "ClÃ­nica", "Hospital", "Dentista", "Fisioterapeuta"
    ],
    "Engenharia": [
        "Engenheiro", "Engenharia", "Civil", "MecÃ¢nico", "ElÃ©trico", "TopÃ³grafo",
        "ConstruÃ§Ã£o", "Estrutural", "PetrÃ³leo", "PetroquÃ­mica", "Minas"
    ],
    "EducaÃ§Ã£o": [
        "Professor", "Professora", "Docente", "Educador", "Formador",
        "Tutor", "Ensino", "Escola", "Universidade", "DocÃªncia"
    ],
    "LogÃ­stica": [
        "Motorista", "LogÃ­stica", "ArmazÃ©m", "Transporte", "Estoca",
        "DistribuiÃ§Ã£o", "Supply Chain", "Compras", "Procurement", "Frota"
    ],
    "Limpeza & ServiÃ§os": [
        "Limpeza", "Higiene", "Lavandaria", "Copeiro", "Cozinheiro",
        "SeguranÃ§a", "Porteiro", "Recepcionista", "Assistente"
    ],
    "Vendas & Marketing": [
        "Vendedor", "Vendas", "Comercial", "Marketing", "Publicidade",
        "RelaÃ§Ãµes PÃºblicas", "Social Media", "E-commerce", "Representante Comercial"
    ],
    "Concurso PÃºblico": [
        "Concurso", "Estado", "Governo", "MinistÃ©rio", "INEFOP", 
        "PÃºblico", "Municipal", "Provincial", "AdministraÃ§Ã£o PÃºblica"
    ],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# JOBS_CONFIG â€” DicionÃ¡rio Unificado de Adaptadores
# Cada chave Ã© o nome do portal. Os valores sÃ£o os seletores CSS especÃ­ficos.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JOBS_CONFIG: Dict[str, dict] = {

    # â”€â”€ 1. ANGOLA EMPREGO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Principal portal local.
    "Angola Emprego": {
        "base_url": "https://www.angola-emprego.com",
        "list_url": "https://www.angola-emprego.com",
        "job_card_selector": ".job-item, .post-item, article",
        "title_selector": ".job-title, h3, h2.entry-title",
        "company_selector": ".company-name, .employer",
        "location_selector": ".location, .city",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description, .entry-content",
        "request_delay_range": (3, 5),
    },

    # â”€â”€ 2. ANGO EMPREGO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "Ango Emprego": {
        "base_url": "https://ango-emprego.com",
        "list_url": "https://ango-emprego.com",
        "job_card_selector": "article, .job_listing",
        "title_selector": "h3, .title",
        "company_selector": ".company, strong",
        "location_selector": ".location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job_description, .content",
        "request_delay_range": (2, 4),
    },

    # â”€â”€ 3. EMPREGA ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "Emprega Angola": {
        "base_url": "https://www.empregaangola.com",
        "list_url": "https://www.empregaangola.com/empregos",
        "job_card_selector": ".job-item, article, .card",
        "title_selector": "h2, h3, .job-title",
        "company_selector": ".company, .employer-name",
        "location_selector": ".location, .province",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description, .vacancy-body",
        "request_delay_range": (2, 4),
    },

    # â”€â”€ 4. ANGOVAGAS.NET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "AngoVagas": {
        "base_url": "https://angovagas.net",
        "list_url": "https://angovagas.net",
        "job_card_selector": "article.post, .post",
        "title_selector": "h2.entry-title, h2",
        "company_selector": ".author, .company",
        "location_selector": ".location, .entry-meta",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".entry-content",
        "request_delay_range": (2, 5),
    },

    # â”€â”€ 5. INEFOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Instituto Nacional do Emprego e FormaÃ§Ã£o Profissional.
    "INEFOP": {
        "base_url": "https://www.inefop.gov.ao",
        "list_url": "https://www.inefop.gov.ao/concursos",
        "job_card_selector": "article, .concurso-item",
        "title_selector": "h1, h2, h3, .entry-title",
        "company_selector": None,
        "location_selector": ".location, .provincia",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".entry-content, article",
        "fixed_company": "Estado Angolano (INEFOP)",
        "fixed_category": "Concurso PÃºblico",
        "request_delay_range": (3, 5),
    },

    # â”€â”€ 6. CAREERJET ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "Careerjet Angola": {
        "base_url": "https://www.careerjet.co.ao",
        "list_url": "https://www.careerjet.co.ao/jobs.html?ISOCountry=AO&locale_code=pt_AO",
        "job_card_selector": "li.job, article.job",
        "title_selector": "h2, .title, a[href*='job']",
        "company_selector": ".company, p.company",
        "location_selector": ".location, .city",
        "link_selector": "header a, h2 a",
        "detail_enabled": False,
        "request_delay_range": (3, 5),
    },

    # â”€â”€ 7. MIRANTES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Foco em Talatona / Luanda Sul.
    "Mirantes": {
        "base_url": "https://www.mirantes.ao",
        "list_url": "https://www.mirantes.ao/emprego",
        "job_card_selector": ".job-listing, article, .post",
        "title_selector": "h2, h3, .job-title",
        "company_selector": ".company, .employer",
        "location_selector": ".location, .city",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description, .entry-content",
        "request_delay_range": (2, 4),
    },

    # â”€â”€ 8. LINKEDIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "LinkedIn": {
        "base_url": "https://www.linkedin.com",
        "list_url": "https://www.linkedin.com/jobs/search/?keywords=angola&location=Angola&f_TPR=r86400",
        "job_card_selector": ".job-search-card, .base-card",
        "title_selector": "h3.base-search-card__title",
        "company_selector": "h4.base-search-card__subtitle",
        "location_selector": ".job-search-card__location",
        "link_selector": "a.base-card__full-link",
        "detail_enabled": False,
        "request_delay_range": (5, 8),
        "extra_headers": {
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
        },
    },
}



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLIENTE SUPABASE REST (SEM supabase-py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SupabaseRestClient:
    def __init__(self, url: str, key: str):
        self.base_url = url.rstrip("/")
        self.headers = {
            "apikey": key,
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json",
            "Prefer": "return=minimal",
        }

    def select(self, table: str, filters: dict = None, columns: str = "*") -> list:
        params = {"select": columns}
        if filters:
            params.update(filters)
        resp = requests.get(
            f"{self.base_url}/rest/v1/{table}",
            headers={**self.headers, "Prefer": ""},
            params=params,
            timeout=10,
        )
        resp.raise_for_status()
        return resp.json()

    def insert(self, table: str, data: dict) -> bool:
        try:
            resp = requests.post(
                f"{self.base_url}/rest/v1/{table}",
                headers=self.headers,
                json=data,
                timeout=10,
            )
            # DepuraÃ§Ã£o solicitada pelo utilizador: Resposta do Supabase
            log.info(f"Resposta do Supabase: {resp.status_code} {resp.text}")
            
            if resp.status_code >= 400:
                log.error(f"âŒ Erro na inserÃ§Ã£o: {resp.text}")
                log.error(f"Payload com erro: {json.dumps(data, ensure_ascii=False)[:300]}")
                return False
            return True
        except Exception as e:
            log.error(f"ğŸ’¥ Falha de conexÃ£o Supabase: {e}")
            return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MOTOR PRINCIPAL â€” AngoJobScraper v2
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AngoJobScraper:
    # Chrome v122 User-Agent â€” contorna a maioria dos bloqueios bÃ¡sicos
    BASE_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "pt-AO,pt;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.google.com/",
        "Cache-Control": "no-cache",
    }

    EMAIL_REGEX = re.compile(
        r"[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}"
    )

    def __init__(self, db: SupabaseRestClient):
        self.db = db
        self.session = requests.Session()
        self.session.headers.update(self.BASE_HEADERS)
        self.stats = {"processed": 0, "saved": 0, "skipped_dup": 0, "errors": 0}

    # â”€â”€ Utilidades â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _clean(self, text: Optional[str]) -> str:
        if not text:
            return ""
        text = unicodedata.normalize("NFKC", text)
        text = re.sub(r"[\x00-\x08\x0b-\x1f\x7f]", "", text)
        return re.sub(r"\s+", " ", text).strip()

    def _normalize_url(self, url: str, base_url: str) -> str:
        if not url:
            return ""
        if url.startswith("http"):
            return url
        return urljoin(base_url, url)

    def _extract_email(self, text: str) -> Optional[str]:
        match = self.EMAIL_REGEX.search(text or "")
        return match.group(0) if match else None

    def _human_delay(self, delay_range: tuple):
        """Simula atraso humano aleatÃ³rio entre requests."""
        secs = random.uniform(*delay_range)
        log.info(f"  â³ Aguardando {secs:.1f}s (simulaÃ§Ã£o humana)...")
        time.sleep(secs)

    def _fetch(self, url: str, extra_headers: dict = None) -> Optional[BeautifulSoup]:
        """Faz o request e retorna BeautifulSoup, ou None se falhar."""
        headers = {}
        if extra_headers:
            headers.update(extra_headers)
        try:
            resp = self.session.get(url, headers=headers, timeout=20)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "html.parser")
        except requests.RequestException as e:
            log.warning(f"  âš ï¸  Falha no request para {url}: {e}")
            return None

    # â”€â”€ ExtraÃ§Ã£o de Imagem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _extract_image(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """Extrai imagem: og:image â†’ primeira img relevante â†’ None."""
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            return og["content"]
        # Procura logo da empresa em imagens
        for img in soup.find_all("img")[:5]:
            src = img.get("src") or img.get("data-src")
            if src and any(kw in src.lower() for kw in ["logo", "company", "employer", "brand"]):
                return self._normalize_url(src, base_url)
        return None

    # â”€â”€ CategorizaÃ§Ã£o AutomÃ¡tica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _categorize(self, title: str, fixed_category: str = None) -> str:
        """Atribui categoria com base em palavras-chave no tÃ­tulo."""
        if fixed_category:
            return fixed_category
        title_lower = title.lower()
        for category, keywords in CATEGORY_MAP.items():
            if any(kw.lower() in title_lower for kw in keywords):
                return category
        return "Geral"

    # â”€â”€ DeduplicaÃ§Ã£o Dupla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _is_duplicate_url(self, source_url: str) -> bool:
        """Verifica se a URL de origem jÃ¡ existe."""
        try:
            res = self.db.select("jobs", filters={"source_url": f"eq.{source_url}"}, columns="id")
            return len(res) > 0
        except Exception:
            return False

    def _is_duplicate_composite(self, title: str, company: str) -> bool:
        """
        DeduplicaÃ§Ã£o inteligente: mesma vaga publicada em mÃºltiplos sites.
        Se tÃ­tulo E empresa forem idÃªnticos, Ã© considerado duplicado.
        """
        if not title or not company or company == "Empresa Confidencial":
            return False
        try:
            res = self.db.select(
                "jobs",
                filters={"title": f"eq.{title}", "company": f"eq.{company}"},
                columns="id",
            )
            return len(res) > 0
        except Exception:
            return False

    # â”€â”€ Auto-DetecÃ§Ã£o de Seletor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _auto_detect_selector(self, soup: BeautifulSoup) -> Optional[str]:
        candidates = [
            "li.job_listing", "article.job_listing", ".job-listing",
            ".job-item", ".vacancy-item", "li.job", ".job_item",
            ".base-card", "article.post", ".post", "article",
        ]
        for sel in candidates:
            items = soup.select(sel)
            if len(items) >= 2:
                log.info(f"  ğŸ” Seletor auto-detectado: '{sel}' ({len(items)} itens)")
                return sel
        return None

    # â”€â”€ Scraper por Site â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def scrape_site(self, site_name: str, cfg: dict):
        """
        Processa um Ãºnico site de forma isolada.
        Falha no site â†’ log de erro â†’ salta para o prÃ³ximo. Nunca para o motor todo.
        """
        log.info(f"\n{'â•' * 60}")
        log.info(f"ğŸ’¼ FONTE: {site_name}")
        log.info(f"   URL: {cfg['list_url']}")
        log.info(f"{'â•' * 60}")

        try:
            soup = self._fetch(cfg["list_url"], extra_headers=cfg.get("extra_headers"))
            if not soup:
                log.error(f"âŒ {site_name} inacessÃ­vel. Saltando para o prÃ³ximo...")
                self.stats["errors"] += 1
                return

            # Tentar seletor configurado, depois auto-detecÃ§Ã£o
            cards = soup.select(cfg["job_card_selector"])
            if not cards:
                log.warning(f"  âš ï¸  Seletor '{cfg['job_card_selector']}' sem resultados. A tentar auto-detecÃ§Ã£o...")
                detected = self._auto_detect_selector(soup)
                if detected:
                    cards = soup.select(detected)
                else:
                    log.error(f"  âŒ NÃ£o foi possÃ­vel encontrar cards em {site_name}. Saltando.")
                    self.stats["errors"] += 1
                    return

            log.info(f"  ğŸ“‹ {len(cards)} cards encontrados. Processando...")

            for card in cards[:15]:  # MÃ¡x 15 por site por ciclo
                self.stats["processed"] += 1
                try:
                    # â”€â”€ Link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    link_tag = card.select_one(cfg["link_selector"]) or card.find("a")
                    raw_url = link_tag.get("href", "") if link_tag else ""
                    job_url = self._normalize_url(raw_url, cfg["base_url"])

                    # â”€â”€ DeduplicaÃ§Ã£o por URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if job_url and self._is_duplicate_url(job_url):
                        log.info(f"  â­ï¸  Duplicado (URL): {job_url[:70]}")
                        self.stats["skipped_dup"] += 1
                        continue

                    # â”€â”€ TÃ­tulo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    title_tag = card.select_one(cfg["title_selector"])
                    title = self._clean(title_tag.get_text() if title_tag else "")
                    if not title or len(title) < 4:
                        continue

                    # â”€â”€ Empresa â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    company = cfg.get("fixed_company", "")
                    if not company and cfg.get("company_selector"):
                        company_tag = card.select_one(cfg["company_selector"])
                        company = self._clean(company_tag.get_text() if company_tag else "")
                    if not company:
                        company = "Empresa Confidencial"

                    # â”€â”€ DeduplicaÃ§Ã£o por (TÃ­tulo + Empresa) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if self._is_duplicate_composite(title, company):
                        log.info(f"  â­ï¸  Duplicado (tÃ­tulo+empresa): {title[:50]} @ {company}")
                        self.stats["skipped_dup"] += 1
                        continue

                    # â”€â”€ LocalizaÃ§Ã£o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    location_tag = card.select_one(cfg["location_selector"]) if cfg.get("location_selector") else None
                    location = self._clean(location_tag.get_text() if location_tag else "Angola")
                    if not location:
                        location = "Angola"

                    # â”€â”€ Detalhe da Vaga â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    description = ""
                    contact_email = None
                    image_url = None

                    if cfg.get("detail_enabled") and job_url:
                        self._human_delay(cfg.get("request_delay_range", (2, 3)))
                        log.info(f"  ğŸ“„ Abrindo detalhe: {title[:50]}...")
                        detail_soup = self._fetch(job_url, extra_headers=cfg.get("extra_headers"))
                        if detail_soup:
                            detail_sel = cfg.get("detail_description_selector", ".entry-content")
                            body = detail_soup.select_one(detail_sel)
                            if body:
                                description = self._clean(body.get_text(separator=" "))[:3000]
                            contact_email = self._extract_email(detail_soup.get_text())
                            image_url = self._extract_image(detail_soup, cfg["base_url"])
                    else:
                        contact_email = self._extract_email(card.get_text())
                        image_url = self._extract_image(card, cfg["base_url"])
                        self._human_delay(cfg.get("request_delay_range", (2, 4)))

                    # â”€â”€ Categoria â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    categoria = self._categorize(title, cfg.get("fixed_category"))

                    # â”€â”€ Payload Supabase (Mantendo fidelidade ao Schema e Front-end) â”€â”€â”€â”€â”€â”€
                    # Imagem e Categoria tratadas como strings para evitar erros de nulo se a coluna for obrigatÃ³ria
                    payload = {
                        "title": title[:255],
                        "company": company[:255],
                        "location": location[:255],
                        "description": description or "",
                        "application_email": contact_email or "",
                        "imagem_url": image_url or "",
                        "source_url": job_url or None,
                        "categoria": categoria or "Geral",
                        "status": "pendente",
                        "posted_at": datetime.now(timezone.utc).isoformat(),
                    }

                    success = self.db.insert("jobs", payload)
                    if success:
                        log.info(f"  âœ… Guardada: [{category}] {title[:55]} @ {company}")
                        self.stats["saved"] += 1
                    else:
                        self.stats["errors"] += 1

                except Exception as card_err:
                    log.warning(f"  âš ï¸  Erro num card de {site_name}: {card_err}")
                    continue

            time.sleep(3)  # Pausa entre sites

        except Exception as site_err:
            # Blindagem total â€” erro no site nunca para o motor
            log.error(f"âŒ SITE FALHADO: {site_name} â€” {site_err}")
            log.error(f"   â†’ A saltar para o prÃ³ximo site...")
            self.stats["errors"] += 1

    # â”€â”€ Loop Principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self):
        """Itera por todos os sites de forma independente."""
        start = datetime.now(timezone.utc)
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  AngoJobScraper v2 â€” SUPER MOTOR DE EMPREGOS")
        log.info(f"  {len(JOBS_CONFIG)} fontes configuradas")
        log.info(f"  {start.strftime('%Y-%m-%d %H:%M:%S UTC')}")
        log.info(f"{'â–ˆ' * 60}\n")

        for site_name, cfg in JOBS_CONFIG.items():
            self.scrape_site(site_name, cfg)

        elapsed = (datetime.now(timezone.utc) - start).seconds
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  ğŸ VARREDURA CONCLUÃDA em {elapsed}s")
        log.info(f"  ğŸ“Š Processados:  {self.stats['processed']}")
        log.info(f"  ğŸ’¾ Guardados:    {self.stats['saved']}")
        log.info(f"  â­ï¸  Duplicados:   {self.stats['skipped_dup']}")
        log.info(f"  âŒ Erros:        {self.stats['errors']}")
        log.info(f"{'â–ˆ' * 60}\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PONTO DE ENTRADA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env.local"))
    SUPABASE_URL = os.getenv("VITE_SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("VITE_SUPABASE_ANON_KEY")

    if not SUPABASE_URL or not SUPABASE_KEY:
        log.critical(
            "âŒ Credenciais Supabase em falta. "
            "Defina VITE_SUPABASE_URL e SUPABASE_SERVICE_ROLE_KEY no .env.local"
        )
        exit(1)

    log.info(f"ğŸ”— Supabase: {SUPABASE_URL}")
    db = SupabaseRestClient(url=SUPABASE_URL, key=SUPABASE_KEY)
    scraper = AngoJobScraper(db=db)
    scraper.run()
