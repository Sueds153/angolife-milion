"""
AngoJobScraper v2 â€” Super Motor de Vagas de Emprego Angolanas
=============================================================
Arquitetura de 'Adaptadores' unificada com suporte a 8+ fontes.

Fontes configuradas:
  1. AngoEmprego.com     â†’ Plataforma nacional lÃ­der
  2. AngoVagas.net       â†’ WordPress, volume mÃ©dio
  3. Emprega Angola      â†’ Portal nacional moderno
  4. INEFOP              â†’ Concursos PÃºblicos e Estado
  5. Careerjet Angola    â†’ Volume de vagas classe mÃ©dia
  6. Mirantes            â†’ Talatona / Luanda Sul
  7. AngoJob.net         â†’ Portal agregador angolano
  8. LinkedIn (PÃºblico)  â†’ Vagas pÃºblicas sem login

Funcionalidades:
  âœ… JOBS_CONFIG â€” dicionÃ¡rio unificado de adaptadores
  âœ… Chrome v122 User-Agent real (anti-403/bloqueios)
  âœ… DeduplicaÃ§Ã£o dupla: por source_url E por (title + company)
  âœ… CategorizaÃ§Ã£o automÃ¡tica por palavras-chave no tÃ­tulo
  âœ… ExtraÃ§Ã£o de imagem: og:image â†’ logo img â†’ None
  âœ… ExtraÃ§Ã£o de e-mail por regex na pÃ¡gina de detalhe
  âœ… 2-5s de delay aleatÃ³rio entre requests (simulaÃ§Ã£o humana)
  âœ… Per-site try-except blindado â€” falha isolada por fonte
  âœ… Log de estatÃ­sticas completo no final

DependÃªncias:
    pip install requests beautifulsoup4 python-dotenv
"""

import re
import os
import time
import json
import random
import logging
import unicodedata
from datetime import datetime, timezone
from typing import Optional, List, Dict
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURAÃ‡ÃƒO DE LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("jobs_scraper.log", encoding="utf-8"),
    ],
)
log = logging.getLogger("AngoJobScraper")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INTELIGÃŠNCIA: CategorizaÃ§Ã£o automÃ¡tica de vagas por tÃ­tulo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_MAP = {
    "Tecnologia": [
        "IT", "TI", "InformÃ¡tica", "Developer", "Desenvolvedor", "Programador",
        "Software", "Sistemas", "Redes", "CiberseguranÃ§a", "Data", "Python", "Java",
        "Frontend", "Backend", "Fullstack", "DevOps", "Cloud", "Suporte TÃ©cnico"
    ],
    "GestÃ£o": [
        "Gerente", "Gestor", "Director", "Diretor", "Manager", "Supervisor",
        "Coordenador", "CoordenaÃ§Ã£o", "CEO", "CFO", "COO", "Chefe", "ResponsÃ¡vel"
    ],
    "FinanÃ§as": [
        "Contabilista", "Contabilidade", "Financeiro", "FinanÃ§as", "Auditor",
        "Auditoria", "Tesoureiro", "Economista", "AnÃ¡lise Financeira", "Fiscal"
    ],
    "SaÃºde": [
        "MÃ©dico", "Enfermeiro", "Enfermeira", "FarmacÃªutico", "TÃ©cnico de SaÃºde",
        "SaÃºde", "ClÃ­nica", "Hospital", "Dentista", "Fisioterapeuta"
    ],
    "Engenharia": [
        "Engenheiro", "Engenharia", "Civil", "MecÃ¢nico", "ElÃ©trico", "TopÃ³grafo",
        "ConstruÃ§Ã£o", "Estrutural", "PetrÃ³leo", "PetroquÃ­mica", "Minas"
    ],
    "EducaÃ§Ã£o": [
        "Professor", "Professora", "Docente", "Educador", "Formador",
        "Tutor", "Ensino", "Escola", "Universidade", "DocÃªncia"
    ],
    "LogÃ­stica": [
        "Motorista", "LogÃ­stica", "ArmazÃ©m", "Transporte", "Estoca",
        "DistribuiÃ§Ã£o", "Supply Chain", "Compras", "Procurement", "Frota"
    ],
    "Limpeza & ServiÃ§os": [
        "Limpeza", "Higiene", "Lavandaria", "Copeiro", "Cozinheiro",
        "SeguranÃ§a", "Porteiro", "Recepcionista", "Assistente"
    ],
    "Vendas & Marketing": [
        "Vendedor", "Vendas", "Comercial", "Marketing", "Publicidade",
        "RelaÃ§Ãµes PÃºblicas", "Social Media", "E-commerce", "Representante Comercial"
    ],
    "Concurso PÃºblico": [
        "Concurso", "Estado", "Governo", "MinistÃ©rio", "INEFOP", 
        "PÃºblico", "Municipal", "Provincial", "AdministraÃ§Ã£o PÃºblica"
    ],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# JOBS_CONFIG â€” DicionÃ¡rio Unificado de Adaptadores
# Cada chave Ã© o nome do portal. Os valores sÃ£o os seletores CSS especÃ­ficos.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# JOBS_CONFIG â€” DicionÃ¡rio Unificado de Adaptadores (Ordenado por Peso)
# 1-2: HTML EstÃ¡tico (Leve) | 3-6: DinÃ¢micos | 7-8: Pesados (LinkedIn/JS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JOBS_CONFIG: Dict[str, dict] = {
    "Portal de Emprego": {
        "base_url": "https://portaldeemprego.ao",
        "list_url": "https://portaldeemprego.ao/vagas/",
        "job_card_selector": ".job-item, .post",
        "title_selector": "h2, .job-title",
        "company_selector": ".job-company",
        "location_selector": ".job-location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description, .entry-content",
        "detail_requirements_selector": ".job-requirements, .entry-content ul",
        "request_delay_range": (1, 3),
    },
    "Ango Emprego": {
        "base_url": "https://angoemprego.com",
        "list_url": "https://angoemprego.com",
        "job_card_selector": "article, .job-listing",
        "title_selector": "h3, .title",
        "company_selector": ".company, strong",
        "location_selector": ".location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job_description, .entry-content",
        "detail_requirements_selector": ".entry-content ul",
        "request_delay_range": (1, 3),
    },
    "AngoVagas": {
        "base_url": "https://angovagas.net",
        "list_url": "https://angovagas.net",
        "job_card_selector": "article.post",
        "title_selector": "h2.entry-title",
        "company_selector": ".author",
        "location_selector": ".entry-meta",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".entry-content",
        "detail_requirements_selector": ".entry-content ul",
        "request_delay_range": (2, 4),
    },
    "INEFOP": {
        "base_url": "https://www.inefop.gov.ao",
        "list_url": "https://www.inefop.gov.ao/concursos",
        "job_card_selector": "article",
        "title_selector": "h1, h2, h3",
        "company_selector": None,
        "location_selector": ".provincia",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".entry-content",
        "detail_requirements_selector": None,
        "fixed_company": "Estado Angolano (INEFOP)",
        "fixed_category": "Concurso PÃºblico",
        "request_delay_range": (3, 5),
    },
    "Emprega Angola": {
        "base_url": "https://empregaangola.com",
        "list_url": "https://empregaangola.com/vagas",
        "job_card_selector": ".job-card, .vaga-item",
        "title_selector": "h3, .title",
        "company_selector": ".company-name",
        "location_selector": ".location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description, #description",
        "detail_requirements_selector": ".requirements, #requirements",
        "request_delay_range": (3, 5),
    },
    "Jobartis": {
        "base_url": "https://www.jobartis.com",
        "list_url": "https://www.jobartis.com/pt/empregos-em-angola",
        "job_card_selector": ".job-item, article",
        "title_selector": "h3, .title",
        "company_selector": ".company",
        "location_selector": ".location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".job-description",
        "detail_requirements_selector": ".job-requirements",
        "request_delay_range": (3, 6),
    },
    "AngoJob": {
        "base_url": "https://angojob.net",
        "list_url": "https://angojob.net/vagas-recentes/",
        "job_card_selector": "article, .post",
        "title_selector": "h2, h3",
        "company_selector": ".company, strong",
        "location_selector": ".location",
        "link_selector": "a",
        "detail_enabled": True,
        "detail_description_selector": ".entry-content",
        "detail_requirements_selector": None,
        "request_delay_range": (2, 4),
    },
    "LinkedIn": {
        "base_url": "https://www.linkedin.com",
        "list_url": "https://www.linkedin.com/jobs/search/?keywords=angola&location=Angola&f_TPR=r86400",
        "job_card_selector": ".job-search-card, .base-card",
        "title_selector": "h3.base-search-card__title",
        "company_selector": "h4.base-search-card__subtitle",
        "location_selector": ".job-search-card__location",
        "link_selector": "a.base-card__full-link",
        "detail_enabled": False, # LinkedIn blockeia scraping de detalhe sem login agressivo
        "request_delay_range": (6, 12),
        "extra_headers": {
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
        },
    },
}



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLIENTE SUPABASE REST (SEM supabase-py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SupabaseRestClient:
    def __init__(self, url: str, key: str):
        self.base_url = url.rstrip("/")
        self.headers = {
            "apikey": key,
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json",
            "Prefer": "return=minimal",
        }

    def select(self, table: str, filters: dict = None, columns: str = "*") -> list:
        params = {"select": columns}
        if filters:
            params.update(filters)
        resp = requests.get(
            f"{self.base_url}/rest/v1/{table}",
            headers={**self.headers, "Prefer": ""},
            params=params,
            timeout=10,
        )
        resp.raise_for_status()
        return resp.json()

    def insert(self, table: str, data: dict) -> bool:
        try:
            resp = requests.post(
                f"{self.base_url}/rest/v1/{table}",
                headers=self.headers,
                json=data,
                timeout=10,
            )
            # DepuraÃ§Ã£o solicitada pelo utilizador: Resposta do Supabase
            log.info(f"Resposta do Supabase: {resp.status_code} {resp.text}")
            
            if resp.status_code >= 400:
                log.error(f"âŒ Erro na inserÃ§Ã£o: {resp.text}")
                log.error(f"Payload com erro: {json.dumps(data, ensure_ascii=False)[:300]}")
                return False
            return True
        except Exception as e:
            log.error(f"ğŸ’¥ Falha de conexÃ£o Supabase: {e}")
            return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MOTOR PRINCIPAL â€” AngoJobScraper v2
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AngoJobScraper:
    # Chrome v122 User-Agent â€” contorna a maioria dos bloqueios bÃ¡sicos
    BASE_HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "pt-AO,pt;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.google.com/",
        "Cache-Control": "no-cache",
    }

    EMAIL_REGEX = re.compile(
        r"[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}"
    )

    def __init__(self, db: SupabaseRestClient):
        self.db = db
        self.session = requests.Session()
        self.session.headers.update(self.BASE_HEADERS)
        self.stats = {"processed": 0, "saved": 0, "skipped_dup": 0, "errors": 0}

    # â”€â”€ Utilidades â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _clean(self, text: Optional[str]) -> str:
        if not text:
            return ""
        text = unicodedata.normalize("NFKC", text)
        text = re.sub(r"[\x00-\x08\x0b-\x1f\x7f]", "", text)
        return re.sub(r"\s+", " ", text).strip()

    def _normalize_url(self, url: str, base_url: str) -> str:
        if not url:
            return ""
        if url.startswith("http"):
            return url
        return urljoin(base_url, url)

    def _extract_email(self, text: str) -> Optional[str]:
        match = self.EMAIL_REGEX.search(text or "")
        return match.group(0) if match else None

    def _human_delay(self, delay_range: tuple):
        """Simula atraso humano aleatÃ³rio entre requests."""
        secs = random.uniform(*delay_range)
        log.info(f"  â³ Aguardando {secs:.1f}s (simulaÃ§Ã£o humana)...")
        time.sleep(secs)

    def _fetch(self, url: str, extra_headers: dict = None) -> Optional[BeautifulSoup]:
        """Faz o request e retorna BeautifulSoup, ou None se falhar."""
        headers = {}
        if extra_headers:
            headers.update(extra_headers)
        try:
            resp = self.session.get(url, headers=headers, timeout=20)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "html.parser")
        except requests.RequestException as e:
            log.warning(f"  âš ï¸  Falha no request para {url}: {e}")
            return None

    # â”€â”€ ExtraÃ§Ã£o de Imagem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _extract_image(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """Extrai imagem: og:image â†’ primeira img relevante â†’ None."""
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            return og["content"]
        # Procura logo da empresa em imagens
        for img in soup.find_all("img")[:5]:
            src = img.get("src") or img.get("data-src")
            if src and any(kw in src.lower() for kw in ["logo", "company", "employer", "brand"]):
                return self._normalize_url(src, base_url)
        return None

    # â”€â”€ CategorizaÃ§Ã£o AutomÃ¡tica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _categorize(self, title: str, fixed_category: str = None) -> str:
        """Atribui categoria com base em palavras-chave no tÃ­tulo."""
        if fixed_category:
            return fixed_category
        title_lower = title.lower()
        for category, keywords in CATEGORY_MAP.items():
            if any(kw.lower() in title_lower for kw in keywords):
                return category
        return "Geral"

    # â”€â”€ DeduplicaÃ§Ã£o Dupla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _is_duplicate_url(self, source_url: str) -> bool:
        """Verifica se a URL de origem jÃ¡ existe."""
        try:
            res = self.db.select("jobs", filters={"source_url": f"eq.{source_url}"}, columns="id")
            return len(res) > 0
        except Exception:
            return False

    def _is_duplicate_composite(self, title: str, company: str) -> bool:
        """
        DeduplicaÃ§Ã£o inteligente: mesma vaga publicada em mÃºltiplos sites.
        Se tÃ­tulo E empresa forem idÃªnticos, Ã© considerado duplicado.
        """
        if not title or not company or company == "Empresa Confidencial":
            return False
        try:
            res = self.db.select(
                "jobs",
                filters={"title": f"eq.{title}", "company": f"eq.{company}"},
                columns="id",
            )
            return len(res) > 0
        except Exception:
            return False

    # â”€â”€ Auto-DetecÃ§Ã£o de Seletor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _auto_detect_selector(self, soup: BeautifulSoup) -> Optional[str]:
        candidates = [
            "li.job_listing", "article.job_listing", ".job-listing",
            ".job-item", ".vacancy-item", "li.job", ".job_item",
            ".base-card", "article.post", ".post", "article",
        ]
        for sel in candidates:
            items = soup.select(sel)
            if len(items) >= 2:
                log.info(f"  ğŸ” Seletor auto-detectado: '{sel}' ({len(items)} itens)")
                return sel
        return None

    # â”€â”€ Loop Principal: Round-Robin (RodÃ­zio) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self, max_total_vagas: int = 100):
        """
        Executa o motor em ciclos: 5 vagas por fonte em cada iteraÃ§Ã£o.
        Garante diversidade de fontes no banco de dados.
        """
        start = datetime.now(timezone.utc)
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  AngoJobScraper v2.5 â€” MODO RODÃZIO ATIVADO")
        log.info(f"  {len(JOBS_CONFIG)} fontes em ciclo | Meta: {max_total_vagas} vagas")
        log.info(f"{'â–ˆ' * 60}\n")

        # Cache de sopas por fonte para nÃ£o pedir a home 1000 vezes
        soups_cache = {}
        processed_links_per_site = {name: set() for name in JOBS_CONFIG}
        indices_per_site = {name: 0 for name in JOBS_CONFIG}

        while self.stats["saved"] < max_total_vagas:
            saved_this_cycle = 0
            
            for site_name, cfg in JOBS_CONFIG.items():
                if self.stats["saved"] >= max_total_vagas:
                    break
                
                log.info(f"ğŸ”„ Ciclo: {site_name} (InÃ­cio no Ã­ndice {indices_per_site[site_name]})")
                
                try:
                    if site_name not in soups_cache:
                        soups_cache[site_name] = self._fetch(cfg["list_url"], cfg.get("extra_headers"))
                    
                    soup = soups_cache[site_name]
                    if not soup:
                        continue
                        
                    cards = soup.select(cfg["job_card_selector"])
                    if not cards:
                        log.warning(f"  âš ï¸  Nenhum card em {site_name}. Tentando auto-deteÃ§Ã£o...")
                        detected = self._auto_detect_selector(soup)
                        if detected: cards = soup.select(detected)
                    
                    if not cards:
                        continue

                    # Pega as prÃ³ximas 5 vagas nÃ£o processadas
                    count_in_cycle = 0
                    current_idx = indices_per_site[site_name]
                    
                    while count_in_cycle < 5 and current_idx < len(cards):
                        card = cards[current_idx]
                        current_idx += 1
                        
                        # ExtraÃ§Ã£o de Link
                        link_tag = card.select_one(cfg["link_selector"]) or card.find("a")
                        raw_url = link_tag.get("href", "") if link_tag else ""
                        job_url = self._normalize_url(raw_url, cfg["base_url"])
                        
                        if not job_url or job_url in processed_links_per_site[site_name]:
                            continue
                        
                        processed_links_per_site[site_name].add(job_url)
                        
                        # Processar Vaga
                        success = self._process_card(card, job_url, site_name, cfg)
                        if success:
                            count_in_cycle += 1
                            saved_this_cycle += 1
                            self.stats["saved"] += 1
                    
                    indices_per_site[site_name] = current_idx
                    
                except Exception as e:
                    log.error(f"âŒ Erro no ciclo de {site_name}: {e}")
                    continue

            if saved_this_cycle == 0:
                log.info("ğŸ Nenhuma nova vaga encontrada em todas as fontes. Finalizando.")
                break
            
            log.info(f"ğŸ“Š Fim do Ciclo. Total guardado: {self.stats['saved']}/{max_total_vagas}")

        elapsed = (datetime.now(timezone.utc) - start).seconds
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  ğŸ VARREDURA CONCLUÃDA em {elapsed}s")
        log.info(f"  ğŸ“Š EstatÃ­sticas Finais:")
        log.info(f"     â†’ Guardados:   {self.stats['saved']}")
        log.info(f"     â†’ Erros:       {self.stats['errors']}")
        log.info(f"{'â–ˆ' * 60}\n")

    def _process_card(self, card, job_url, site_name, cfg) -> bool:
        """ExtraÃ§Ã£o e inserÃ§Ã£o de uma Ãºnica vaga."""
        try:
            # 1. DeduplicaÃ§Ã£o URL
            if self._is_duplicate_url(job_url):
                return False

            # 2. TÃ­tulo & Empresa (ObrigatÃ³rios)
            title_tag = card.select_one(cfg["title_selector"])
            title = self._clean(title_tag.get_text() if title_tag else "")
            
            company = cfg.get("fixed_company", "")
            if not company and cfg.get("company_selector"):
                comp_tag = card.select_one(cfg["company_selector"])
                company = self._clean(comp_tag.get_text() if comp_tag else "")
            if not company: company = "Empresa Confidencial"

            if not title or not company:
                return False

            # 3. LocalizaÃ§Ã£o
            loc_tag = card.select_one(cfg["location_selector"]) if cfg.get("location_selector") else None
            location = self._clean(loc_tag.get_text() if loc_tag else "Angola")

            # 4. DEEP SCRAPING (PÃ¡gina de Detalhe)
            description = ""
            requirements_list = []
            image_url = ""
            email = ""

            if cfg.get("detail_enabled") and job_url:
                self._human_delay(cfg.get("request_delay_range", (2, 4)))
                detail_soup = self._fetch(job_url, cfg.get("extra_headers"))
                if detail_soup:
                    # DescriÃ§Ã£o
                    desc_sel = cfg.get("detail_description_selector")
                    desc_tag = detail_soup.select_one(desc_sel) if desc_sel else None
                    if desc_tag:
                        description = self._clean(desc_tag.get_text(separator="\n"))
                    
                    # Requisitos (Convertendo para Lista)
                    req_sel = cfg.get("detail_requirements_selector")
                    req_tag = detail_soup.select_one(req_sel) if req_sel else None
                    if req_tag:
                        req_text = self._clean(req_tag.get_text(separator="\n"))
                        # Split por quebra de linha, filtra vazios e limpa espaÃ§os
                        requirements_list = [r.strip("- â€¢").strip() for r in req_text.split("\n") if r.strip()]
                    
                    # Imagem/Logo
                    image_url = self._extract_image(detail_soup, cfg["base_url"])
                    
                    # Email por Regex na descriÃ§Ã£o profunda
                    email = self._extract_email(detail_soup.get_text())
            
            # 5. Fallbacks e Limpeza
            if not image_url:
                image_url = self._get_category_placeholder(title)
            
            if not email:
                # Se nÃ£o houver email, guardamos o link de candidatura
                email = f"Candidatar via: {job_url}"

            categoria = self._categorize(title, cfg.get("fixed_category"))

            payload = {
                "title": title[:255],
                "company": company[:255],
                "location": location[:255],
                "description": description[:5000],
                "requirements": requirements_list, # Enviado como ARRAY JSON para o Supabase
                "application_email": email[:255],
                "imagem_url": image_url,
                "source_url": job_url,
                "categoria": categoria,
                "status": "pendente",
                "posted_at": datetime.now(timezone.utc).isoformat(),
            }

            return self.db.insert("jobs", payload)

        except Exception as e:
            log.warning(f"  âš ï¸ Erro ao processar card: {e}")
            self.stats["errors"] += 1
            return False

    def _get_category_placeholder(self, title: str) -> str:
        """Retorna uma imagem por categoria se o logo nÃ£o for encontrado."""
        cat = self._categorize(title)
        placeholders = {
            "Tecnologia": "https://img.icons8.com/color/144/code.png",
            "GestÃ£o": "https://img.icons8.com/color/144/manager.png",
            "FinanÃ§as": "https://img.icons8.com/color/144/money-bag-lira.png",
            "SaÃºde": "https://img.icons8.com/color/144/hospital.png",
            "Engenharia": "https://img.icons8.com/color/144/engineering.png",
            "Vendas & Marketing": "https://img.icons8.com/color/144/megaphone.png",
            "Geral": "https://img.icons8.com/color/144/company.png"
        }
        return placeholders.get(cat, placeholders["Geral"])


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PONTO DE ENTRADA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env.local"))
    SUPABASE_URL = os.getenv("VITE_SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("VITE_SUPABASE_ANON_KEY")

    if not SUPABASE_URL or not SUPABASE_KEY:
        log.critical(
            "âŒ Credenciais Supabase em falta. "
            "Defina VITE_SUPABASE_URL e SUPABASE_SERVICE_ROLE_KEY no .env.local"
        )
        exit(1)

    log.info(f"ğŸ”— Supabase: {SUPABASE_URL}")
    db = SupabaseRestClient(url=SUPABASE_URL, key=SUPABASE_KEY)
    scraper = AngoJobScraper(db=db)
    scraper.run()
