"""
AngoNewsScraper v2 â€” Motor de AgregaÃ§Ã£o Robusta de NotÃ­cias Angolanas
=====================================================================
Arquitetura de 'Adaptadores' com suporte a 10+ fontes independentes.

Fontes configuradas:
  1. ExpansÃ£o          â†’ Economia
  2. Jornal de Angola  â†’ Angola / Geral
  3. TPA               â†’ Oficial / Urgente
  4. TV Girassol       â†’ Oficial
  5. ANGOP             â†’ Urgente / Oficial
  6. Novo Jornal       â†’ InvestigaÃ§Ã£o / Sociedade
  7. NovaGazeta        â†’ Utilidade / Cotidiano
  8. Rede Angola       â†’ Independente / Cultura
  9. TopAngola         â†’ Lifestyle / Diversificado
 10. XÃ© Angola        â†’ Sociedade / Entretenimento
 11. AngonotÃ­cias     â†’ Geral
 12. PlatinaLine       â†’ Geral

Funcionalidades:
  âœ… SITES_CONFIG â€” dicionÃ¡rio global de adaptadores CSS
  âœ… Chrome User-Agent real (anti-403)
  âœ… NormalizaÃ§Ã£o de URLs relativas
  âœ… ExtraÃ§Ã£o de imagem em 3 nÃ­veis (og:image â†’ img â†’ placeholder)
  âœ… Flags de UrgÃªncia (is_priority) e categoria automÃ¡tica
  âœ… Loop independente com try-except por site
  âœ… DeduplicaÃ§Ã£o por url_origem antes do insert no Supabase

DependÃªncias:
    pip install requests beautifulsoup4 python-dotenv
"""

import re
import os
import time
import json
import logging
import unicodedata
from datetime import datetime, timezone
from typing import Optional, List, Dict
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURAÃ‡ÃƒO DE LOGGING
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("news_scraper.log", encoding="utf-8"),
    ],
)
log = logging.getLogger("AngoNewsScraper")

ANGOLIFE_PLACEHOLDER = "https://angolife.app/placeholder-news.jpg"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INTELIGÃŠNCIA: Palavras-chave para categorizaÃ§Ã£o e prioridade
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PRIORITY_KEYWORDS = [
    'Ãšltima Hora', 'Urgente', 'Flash', 'BNA', 'Kwanza',
    'Breaking', 'Alerta', 'AtenÃ§Ã£o', 'Mandato', 'Crise'
]
OPPORTUNITY_KEYWORDS = [
    'Concurso', 'Estado', 'AdmissÃ£o', 'Bolsa', 'Recrutamento',
    'Vaga', 'Emprego', 'EstÃ¡gio', 'Candidatura'
]
ECONOMY_KEYWORDS = [
    'Kwanza', 'BNA', 'CÃ¢mbio', 'InflaÃ§Ã£o', 'Bancos', 'PetrÃ³leo',
    'PIB', 'FMI', 'Economia', 'Mercado', 'DÃ­vida', 'Crescimento'
]
CULTURE_KEYWORDS = [
    'Cultura', 'Arte', 'MÃºsica', 'Festival', 'Cinema', 'Literatura',
    'Futebol', 'Sport', 'Desporto', 'Entretenimento'
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SITES_CONFIG â€” DicionÃ¡rio Global de Adaptadores
# Cada entrada Ã© um portal independente com os seus prÃ³prios seletores CSS.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SITES_CONFIG: Dict[str, dict] = {

    # â”€â”€ 1. EXPANSÃƒO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Foco em Economia. Estrutura baseada em artigos padrÃ£o WordPress.
    "ExpansÃ£o": {
        "base_url": "https://www.expansao.co.ao",
        "list_url": "https://www.expansao.co.ao/economia.html",
        "article_selector": "article, .detalhe, .K2Teaser",
        "title_selector": "h1, h2, h3, .titulo, .itemTitle",
        "link_selector": "a",
        "fixed_category": "Economia",
    },

    # â”€â”€ 2. JORNAL DE ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Portal official. Estrutura com cards de notÃ­cias.
    "Jornal de Angola": {
        "base_url": "https://www.jornaldeangola.ao",
        "list_url": "https://www.jornaldeangola.ao/ao/noticias/",
        "article_selector": "article, .news-card, .td-module-container",
        "title_selector": "h1, h2, h3, .title, .entry-title, .td-module-title",
        "link_selector": "a",
        "fixed_category": "Angola",
    },

    # â”€â”€ 3. TPA (TelevisÃ£o PÃºblica de Angola) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # NotÃ­cias oficiais. Foco em destaques.
    "TPA": {
        "base_url": "https://www.tpa.ao",
        "list_url": "https://www.tpa.ao/noticias",
        "article_selector": "article, .news-item, .post",
        "title_selector": "h1, h2, h3, .post-title, .entry-title",
        "link_selector": "a",
        "fixed_category": "Oficial",
    },

    # â”€â”€ 4. TV GIRASSOL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # NotÃ­cias e entretenimento oficial.
    "TV Girassol": {
        "base_url": "https://www.tvgirassol.com",
        "list_url": "https://www.tvgirassol.com/noticias",
        "article_selector": "article, .news-card, .jeg_post",
        "title_selector": "h1, h2, h3, .jeg_post_title, .post-title",
        "link_selector": "a",
        "fixed_category": "Oficial",
    },

    # â”€â”€ 5. ANGOP (AgÃªncia Angola Press) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # AgÃªncia oficial de notÃ­cias. Foco em Urgente e Oficial.
    "ANGOP": {
        "base_url": "https://www.angop.ao",
        "list_url": "https://www.angop.ao/angola/pt_pt/noticias/",
        "article_selector": ".news_item, article, .item",
        "title_selector": "h1, h2, h3, .item-title, .news-title",
        "link_selector": "a",
        "fixed_category": "Urgente",
    },

    # â”€â”€ 6. NOVO JORNAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Foco em InvestigaÃ§Ã£o e Sociedade. Fortemente anti-scraping â†’ Chrome UA obrigatÃ³rio.
    "Novo Jornal": {
        "base_url": "https://www.novojornal.co.ao",
        "list_url": "https://www.novojornal.co.ao/sociedade/",
        "article_selector": "article, .td-module-container, .jeg_post",
        "title_selector": "h1, h2, h3, .td-module-title, .jeg_post_title",
        "link_selector": "a",
        "fixed_category": "InvestigaÃ§Ã£o",
    },

    # â”€â”€ 7. NOVA GAZETA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # VersÃ£o digital. ConteÃºdo de utilidade pÃºblica e cotidiano.
    "NovaGazeta": {
        "base_url": "https://novagazeta.co.ao",
        "list_url": "https://novagazeta.co.ao/category/noticias/",
        "article_selector": "article, .post, .news-item",
        "title_selector": "h1, h2, h3, .entry-title, .post-title",
        "link_selector": "a",
        "fixed_category": "Utilidade",
    },

    # â”€â”€ 8. REDE ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # NotÃ­cias independentes, anÃ¡lise e cultura angolana.
    "Rede Angola": {
        "base_url": "https://www.redeangola.info",
        "list_url": "https://www.redeangola.info/noticias/",
        "article_selector": "article, .entry, .hentry",
        "title_selector": "h1, h2, h3, .entry-title",
        "link_selector": "a",
        "fixed_category": "Cultura",
    },

    # â”€â”€ 9. TOP ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ConteÃºdo diversificado, lifestyle e tendÃªncias.
    "TopAngola": {
        "base_url": "https://topangola.com",
        "list_url": "https://topangola.com/noticias/",
        "article_selector": "article, .jeg_post, .post-item",
        "title_selector": "h1, h2, h3, .jeg_post_title, .entry-title",
        "link_selector": "a",
        "fixed_category": "Lifestyle",
    },

    # â”€â”€ 10. XÃ‰ ANGOLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Grande alcance em sociedade e entretenimento popular.
    "XÃ© Angola": {
        "base_url": "https://xeangola.com",
        "list_url": "https://xeangola.com/noticias/",
        "article_selector": "article, .post, .jeg_post",
        "title_selector": "h1, h2, h3, .entry-title, .jeg_post_title",
        "link_selector": "a",
        "fixed_category": "Entretenimento",
    },

    # â”€â”€ 11. ANGONOTÃCIAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "AngonotÃ­cias": {
        "base_url": "https://www.angonoticias.com",
        "list_url": "https://www.angonoticias.com/Artigos/canal/2/generalista",
        "article_selector": ".noticia, article, .item",
        "title_selector": "h1, h2, h3, .titulo",
        "link_selector": "a",
        "fixed_category": "Angola",
    },

    # â”€â”€ 12. PLATINALINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "PlatinaLine": {
        "base_url": "https://platinaline.com",
        "list_url": "https://platinaline.com/category/noticias/",
        "article_selector": "article",
        "title_selector": "h1, h2, h3, h4, .post-title",
        "link_selector": "a",
        "fixed_category": "Geral",
    },
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLIENTE SUPABASE REST (REUTILIZADO)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SupabaseRestClient:
    def __init__(self, url: str, key: str):
        self.base_url = url.rstrip("/")
        self.headers = {
            "apikey": key,
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json",
            "Prefer": "return=minimal",
        }

    def select(self, table: str, filters: dict = None, columns: str = "*") -> list:
        params = {"select": columns}
        if filters:
            params.update(filters)
        resp = requests.get(
            f"{self.base_url}/rest/v1/{table}",
            headers={**self.headers, "Prefer": ""},
            params=params,
            timeout=10,
        )
        resp.raise_for_status()
        return resp.json()

    def insert(self, table: str, data: dict) -> bool:
        try:
            resp = requests.post(
                f"{self.base_url}/rest/v1/{table}",
                headers=self.headers,
                json=data,
                timeout=10,
            )
            # DepuraÃ§Ã£o solicitada pelo utilizador: Resposta do Supabase
            log.info(f"Resposta do Supabase: {resp.status_code} {resp.text}")
            
            if resp.status_code >= 400:
                log.error(f"âŒ Erro na inserÃ§Ã£o: {resp.text}")
                log.error(f"Payload com erro: {json.dumps(data, ensure_ascii=False)[:500]}")
                return False
            return True
        except Exception as e:
            log.error(f"ğŸ’¥ Falha de conexÃ£o Supabase: {e}")
            return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MOTOR PRINCIPAL - CLASSE AngoNewsScraper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AngoNewsScraper:
    def __init__(self, db: SupabaseRestClient):
        self.db = db
        # SessÃ£o com User-Agent real Chrome 122 â€” evita bloqueios 403
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "pt-AO,pt;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        })
        self.stats = {"processed": 0, "saved": 0, "skipped_dup": 0, "errors": 0}

    # â”€â”€ NormalizaÃ§Ã£o de URLs relativas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def normalize_url(self, url: str, base_url: str) -> str:
        """Converte links relativos para absolutos usando o domÃ­nio base."""
        if not url:
            return ""
        if url.startswith("http"):
            return url
        return urljoin(base_url, url)

    # â”€â”€ ExtraÃ§Ã£o de Imagem em 3 NÃ­veis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def extract_image(self, soup: BeautifulSoup, base_url: str, content_selector: str = None) -> str:
        """
        NÃ­vel 1: og:image (meta tag â€” mais confiÃ¡vel)
        NÃ­vel 2: Primeira <img> dentro do conteÃºdo principal
        NÃ­vel 3: Placeholder AngoLife
        """
        # NÃ­vel 1: og:image
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            return og["content"]

        # NÃ­vel 2: Primeira imagem no conteÃºdo principal
        content_area = None
        if content_selector:
            content_area = soup.select_one(content_selector)
        if not content_area:
            content_area = soup.find(["article", "main", ".content", ".post-content", ".entry-content"])

        if content_area:
            img = content_area.find("img")
            if img:
                src = img.get("src") or img.get("data-src") or img.get("data-lazy-src")
                if src:
                    return self.normalize_url(src, base_url)

        # NÃ­vel 3: Placeholder AngoLife (Tratamento de Nulos)
        return ANGOLIFE_PLACEHOLDER

    # â”€â”€ ClassificaÃ§Ã£o Inteligente â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def classify(self, title: str, fixed_category: str) -> tuple:
        """
        Retorna (categoria_final, is_priority).
        - Verifica palavras de urgÃªncia â†’ is_priority = True
        - Verifica palavras de oportunidade â†’ categoria = 'Oportunidades' (override)
        - Verifica palavras de economia â†’ categoria = 'Economia' (override)
        - Caso contrÃ¡rio, usa a categoria fixa do adaptador.
        """
        title_normal = title  # mantÃ©m acentos para matching
        is_priority = any(kw.lower() in title_normal.lower() for kw in PRIORITY_KEYWORDS)
        
        # Override de categoria
        if any(kw.lower() in title_normal.lower() for kw in OPPORTUNITY_KEYWORDS):
            return "Oportunidades", is_priority
        if any(kw.lower() in title_normal.lower() for kw in ECONOMY_KEYWORDS):
            return "Economia", is_priority
        if any(kw.lower() in title_normal.lower() for kw in CULTURE_KEYWORDS):
            return "Cultura", is_priority

        return fixed_category, is_priority

    # â”€â”€ Resumo do Texto â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_summary(self, text: str, max_len: int = 220) -> str:
        clean = re.sub(r"\s+", " ", text).strip()
        return (clean[:max_len] + "...") if len(clean) > max_len else clean

    # â”€â”€ SanitizaÃ§Ã£o HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def sanitize_html(self, soup_obj) -> str:
        for tag in soup_obj(["script", "style", "iframe", "ins", "nav", "footer", "aside", "form"]):
            tag.decompose()
        return str(soup_obj)

    # â”€â”€ DeduplicaÃ§Ã£o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def is_duplicate(self, url: str) -> bool:
        """Verifica se a url_origem jÃ¡ estÃ¡ na base de dados."""
        try:
            res = self.db.select("news_articles", filters={"url_origem": f"eq.{url}"}, columns="id")
            return len(res) > 0
        except Exception:
            return False

    # â”€â”€ Scraper por Adaptador â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def scrape_site(self, site_name: str, cfg: dict):
        """
        Processa um Ãºnico site com blindagem try-except.
        Se falhar, imprime o erro no log e passa ao prÃ³ximo site.
        """
        log.info(f"\n{'â•' * 60}")
        log.info(f"ğŸŒ SITE: {site_name} | {cfg['list_url']}")
        log.info(f"{'â•' * 60}")

        try:
            resp = self.session.get(cfg["list_url"], timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            articles = soup.select(cfg["article_selector"])[:12]  # MÃ¡x 12 por ciclo
            if not articles:
                log.warning(f"  âš ï¸  Nenhum artigo encontrado. Seletor: '{cfg['article_selector']}'. Saltando.")
                self.stats["errors"] += 1
                return

            log.info(f"  ğŸ“‹ {len(articles)} artigos encontrados. Processando...")

            for art in articles:
                self.stats["processed"] += 1
                try:
                    # â”€â”€ ExtraÃ§Ã£o do Link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    link_tag = art.select_one(cfg["link_selector"])
                    raw_url = link_tag.get("href", "") if link_tag else ""
                    article_url = self.normalize_url(raw_url, cfg["base_url"])

                    if not article_url or article_url == cfg["base_url"]:
                        continue

                    # â”€â”€ DeduplicaÃ§Ã£o â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if self.is_duplicate(article_url):
                        log.info(f"  â­ï¸  JÃ¡ existe: {article_url[:70]}")
                        self.stats["skipped_dup"] += 1
                        continue

                    # â”€â”€ ExtraÃ§Ã£o do TÃ­tulo (do card de lista) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    title_tag = art.select_one(cfg["title_selector"])
                    title = title_tag.get_text(strip=True) if title_tag else ""

                    if not title or len(title) < 5:
                        continue

                    log.info(f"  âœ¨ Capturando: {title[:65]}...")

                    # â”€â”€ Busca Detalhe do Artigo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    detail_resp = self.session.get(article_url, timeout=15)
                    detail_resp.raise_for_status()
                    detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                    # TÃ­tulo mais preciso vindo da pÃ¡gina de detalhe
                    detail_title_tag = detail_soup.select_one("h1, .entry-title, .article-title")
                    final_title = detail_title_tag.get_text(strip=True) if detail_title_tag else title
                    if not final_title or len(final_title) < 5:
                        final_title = title

                    # â”€â”€ ExtraÃ§Ã£o de Imagem (3 nÃ­veis) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    image_url = self.extract_image(detail_soup, cfg["base_url"])

                    # â”€â”€ ExtraÃ§Ã£o do Corpo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    body_area = detail_soup.select_one(
                        "article, .entry-content, .post-content, .content-body, "
                        ".article-content, .td-post-content, main"
                    )
                    body_html = self.sanitize_html(body_area) if body_area else ""
                    body_text = body_area.get_text(separator=" ") if body_area else detail_soup.get_text()
                    summary = self.get_summary(body_text)

                    # â”€â”€ ClassificaÃ§Ã£o e Prioridade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    categoria, is_priority = self.classify(final_title, cfg.get("fixed_category", "Geral"))

                    # â”€â”€ Payload para Supabase (Check de Nulos e Colunas) â”€â”€â”€â”€â”€
                    payload = {
                        "titulo": final_title[:500],
                        "resumo": (summary or "")[:1000],
                        "corpo": (body_html or "")[:50000],
                        "imagem_url": image_url or ANGOLIFE_PLACEHOLDER,
                        "categoria": categoria or "Geral",
                        "fonte": site_name,
                        "url_origem": article_url,
                        "is_priority": bool(is_priority),
                        "status": "pendente",
                    }

                    success = self.db.insert("news_articles", payload)
                    if success:
                        label = "ğŸ”´ URGENTE" if is_priority else "âœ…"
                        log.info(f"    {label} Guardada | Cat: {categoria} | Prio: {is_priority}")
                        self.stats["saved"] += 1
                    else:
                        self.stats["errors"] += 1

                    time.sleep(1.5)  # Respeito ao servidor entre artigos

                except Exception as art_err:
                    log.warning(f"  âš ï¸  Erro num artigo de {site_name}: {art_err}")
                    continue  # Salta para o prÃ³ximo artigo, nÃ£o para o prÃ³ximo site

            time.sleep(3)  # Pausa entre sites

        except Exception as site_err:
            # Blindagem total: mesmo que o site fique inacessÃ­vel, continua para o prÃ³ximo
            log.error(f"âŒ SITE FALHADO: {site_name} | Erro: {site_err}")
            log.error(f"   â†’ Saltando para o prÃ³ximo site...")
            self.stats["errors"] += 1

    # â”€â”€ Loop Principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self):
        """Itera por todos os sites de forma independente."""
        start_time = datetime.now(timezone.utc)
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  AngoNewsScraper v2 â€” INICIANDO VARREDURA")
        log.info(f"  {len(SITES_CONFIG)} fontes configuradas")
        log.info(f"  {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
        log.info(f"{'â–ˆ' * 60}\n")

        for site_name, cfg in SITES_CONFIG.items():
            self.scrape_site(site_name, cfg)

        elapsed = (datetime.now(timezone.utc) - start_time).seconds
        log.info(f"\n{'â–ˆ' * 60}")
        log.info(f"  âœ… VARREDURA CONCLUÃDA em {elapsed}s")
        log.info(f"  ğŸ“Š Processados:  {self.stats['processed']}")
        log.info(f"  ğŸ’¾ Guardados:    {self.stats['saved']}")
        log.info(f"  â­ï¸  Duplicados:   {self.stats['skipped_dup']}")
        log.info(f"  âŒ Erros:        {self.stats['errors']}")
        log.info(f"{'â–ˆ' * 60}\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PONTO DE ENTRADA
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env.local"))
    SUPABASE_URL = os.getenv("VITE_SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or os.getenv("VITE_SUPABASE_ANON_KEY")

    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("âŒ Credenciais Supabase em falta. Defina VITE_SUPABASE_URL e SUPABASE_SERVICE_ROLE_KEY no .env.local")
        exit(1)

    db_client = SupabaseRestClient(SUPABASE_URL, SUPABASE_KEY)
    scraper = AngoNewsScraper(db_client)
    scraper.run()
